{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from pathlib import Path\n",
    "\n",
    "from utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 13:36:33.925473: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-11 13:36:49.331484: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "model = tf.saved_model.load('yamnet')\n",
    "class_map_path = model.class_map_path().numpy()\n",
    "class_names = class_names_from_csv(class_map_path)\n",
    "\n",
    "emotion_labels = ['Happy music',\n",
    " 'Sad music',\n",
    " 'Tender music',\n",
    " 'Exciting music',\n",
    " 'Angry music',\n",
    " 'Scary music']\n",
    "music_labels = ['Music', \n",
    "                'Musical instrument',\n",
    "                'Singing',\n",
    "                'Drum',\n",
    "                'Rapping']\n",
    "non_music_labels = ['Silence',\n",
    "                    'Speech',\n",
    "                   'Narration, monologue',\n",
    "                    'Chatter',\n",
    "                   'Cheering',\n",
    "                   'Applause',\n",
    "                    ]\n",
    "neutral_labels = []\n",
    "labels = music_labels+non_music_labels+neutral_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label2marker = {'music': '>', 'applause': 's', 'intro': '<', 'interlude': 'D', 'speech': 'X', 'comeback': 'P', 'silence': 'o'}\n",
    "label2color = {'music': 'r', 'applause': 'b', 'intro': 'c', 'interlude': 'k', 'speech': 'm', 'comeback': 'g', 'silence': 'y'}\n",
    "def plot(t_shift=0., music_only=False, non_music_only=False):\n",
    "    fig, axs = plt.subplots(3,1, figsize=(10, 5), gridspec_kw={'height_ratios': [1, 2, 4]})\n",
    "\n",
    "    t = np.arange(0,len(waveform)/16000,1/16000)\n",
    "    # Plot the waveform.\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(t, waveform, linewidth=.2)\n",
    "    \n",
    "    axs[0].get_xaxis().set_visible(False)\n",
    "    axs[0].get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.xlim([t[0], t[-1]])\n",
    "\n",
    "    # Plot the log-mel spectrogram (returned by the model).\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.imshow(spec.T, aspect='auto', interpolation='nearest', origin='lower', cmap='inferno')\n",
    "    axs[1].set_xticklabels([time_format((x*0.01)+t_shift) for x in axs[1].get_xticks()])\n",
    "    axs[1].get_xaxis().set_visible(False)\n",
    "    axs[1].get_yaxis().set_visible(False)\n",
    "\n",
    "    # Plot and label the model output scores for the selected classes.\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.imshow(probs.T, aspect='auto', interpolation='nearest', cmap='gray_r')\n",
    "\n",
    "    patch_padding = (0.025 / 2) / 0.01\n",
    "    plt.xlim([-patch_padding-0.5, probs.shape[0] + patch_padding-0.5])\n",
    "    yticks = range(0, len(labels), 1)\n",
    "    plt.yticks(yticks, labels)\n",
    "    _ = plt.ylim(-0.5 + np.array([len(labels), 0]))\n",
    "    axs[2].set_xticklabels([time_format((x*0.48)+t_shift)[:-4] for x in axs[2].get_xticks()])\n",
    "    plt.xlabel('time [s]')\n",
    "    try:\n",
    "        xmin, xmax, ymin, ymax = axs[1].axis()\n",
    "        for t_start, label in zip(predicted_onsets, predicted_labels):\n",
    "            if label not in ['music'] and music_only:\n",
    "                continue\n",
    "            \n",
    "            if label == 'music' and non_music_only:\n",
    "                continue  \n",
    "                \n",
    "            t_start -= t_shift\n",
    "            if t_start > t[-1]:\n",
    "                break\n",
    "            if t_start < t[0]:\n",
    "                continue\n",
    "            axs[1].vlines(t_start/0.01, ymin, ymax, colors='white', linewidth=1)\n",
    "            axs[1].scatter(t_start/0.01, (ymin+(ymax-ymin)/2), s=64, c='white', marker=label2marker[label])\n",
    "    \n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def processAudioFile(path):\n",
    "    wav_file_name = path\n",
    "    filename = str(wav_file_name).split('/')[-1]\n",
    "    print(filename)\n",
    "    sample_rate, wav_data = wavfile.read(wav_file_name, 'rb')\n",
    "    original_sample_rate = sample_rate\n",
    "    sample_rate, wav_data = ensure_sample_rate(sample_rate, wav_data)\n",
    "    \n",
    "    # Show some basic information about the audio.\n",
    "    duration = len(wav_data)/sample_rate\n",
    "    print(f'Sample rate: {sample_rate} Hz')\n",
    "    print(f'Total duration: {duration:.2f}s')\n",
    "    print(f'Size of the input: {len(wav_data)}')\n",
    "\n",
    "    # # Listening to the wav file.\n",
    "    # Audio(wav_data, rate=sample_rate)\n",
    "    waveform = wav_data / tf.int16.max\n",
    "    # Run the model, check the output.\n",
    "    scores, embeddings, spectrogram = model(waveform)\n",
    "    # 51 scores per 48 samples (3ms)\n",
    "    scores_np = scores.numpy()\n",
    "    spectrogram_np = spectrogram.numpy()\n",
    "    class_indices = []\n",
    "    for l in labels:\n",
    "        class_indices.append(class_names.index(l))\n",
    "    probs = normalize_matrix(scores_np[:, class_indices])\n",
    "    return filename, waveform, spectrogram_np, probs\n",
    "\n",
    "def findMusicBorders(probs, dt = 0.48):\n",
    "    music_starts = []\n",
    "    music_stops = []\n",
    "    \n",
    "    consecutive_count = 0\n",
    "    find_start = True\n",
    "    find_stop = False\n",
    "    \n",
    "    patience = 0\n",
    "    L_music = int(20/dt)\n",
    "    L_nomusic = int(4/dt)\n",
    "    L_patience = int(4/dt)\n",
    "    music_count = 0\n",
    "    nomusic_count = 0\n",
    "    consecutive_count = 0\n",
    "    music_indexes = [labels.index(l) for l in music_labels]\n",
    "    non_music_indexes = [labels.index(l) for l in non_music_labels]\n",
    "    for i, prob in enumerate(probs):\n",
    "        music_prob = np.sum(prob[music_indexes])\n",
    "        non_music_prob = np.sum(prob[non_music_indexes])\n",
    "        ratio = music_prob/non_music_prob\n",
    "        if find_start:\n",
    "            if music_prob > 0.3:\n",
    "                music_count += 1\n",
    "                if music_count == L_music:\n",
    "                    music_starts.append((i-L_music-patience+1)*dt)\n",
    "                    find_start = False\n",
    "                    find_stop = True\n",
    "                    music_count = 0\n",
    "            elif music_count>0:\n",
    "                patience +=1\n",
    "                if patience == L_patience:\n",
    "                    patience = 0 \n",
    "                    music_count = 0\n",
    "            else:\n",
    "                music_count = 0\n",
    "        if find_stop:\n",
    "            if music_prob < 0.1:\n",
    "                nomusic_count += 1\n",
    "                if nomusic_count == L_nomusic:\n",
    "                    music_stops.append((i-L_nomusic+1)*dt)\n",
    "                    find_stop = False\n",
    "                    find_start = True\n",
    "                    nomusic_count = 0\n",
    "            else:\n",
    "                nomusic_count = 0\n",
    "    return music_starts, music_stops\n",
    "\n",
    "def Xtrack(probs, dt=0.48, MUSIC_START_OFFSET = -0.1, MUSIC_STOP_OFFSET = 4.):\n",
    "    T = len(probs)*dt\n",
    "    music_starts, music_stops = findMusicBorders(probs)      \n",
    "    all_onsets = sorted(music_starts+music_stops+[T])\n",
    "    all_labels = []\n",
    "    to_remove = []\n",
    "    for i, o in enumerate(all_onsets[:-1]):\n",
    "        next_onset = all_onsets[i+1]\n",
    "        if o in music_starts:\n",
    "            all_labels.append('music')\n",
    "            if all_onsets[i]+MUSIC_START_OFFSET>0:\n",
    "                all_onsets[i] += MUSIC_START_OFFSET\n",
    "        \n",
    "        if o in music_stops:\n",
    "            all_labels.append('applause')\n",
    "            all_onsets[i] += MUSIC_STOP_OFFSET\n",
    "            \n",
    "            if len(np.where(np.sum(probs[0:15, [labels.index(x) for x in ['Speech', 'Narration, monologue']]], axis=1)>0.5)[0]) > 10/dt:\n",
    "                all_labels[-1] = 'speech' \n",
    "                \n",
    "            if np.mean(probs[int(o/dt):int(next_onset/dt), labels.index('Silence')])>0.5:\n",
    "                all_labels[-1] = 'silence'\n",
    "            \n",
    "        if i > 1:\n",
    "            if all_onsets[i-1] >= all_onsets[i]:\n",
    "                to_remove.append(i)\n",
    "    all_labels = [all_labels[i] for i in range(len(all_labels)) if i not in to_remove]\n",
    "    all_onsets = [all_onsets[i] for i in range(len(all_onsets)) if i not in to_remove]\n",
    "       \n",
    "    \n",
    "    return all_onsets[:-1], all_labels\n",
    "\n",
    "def writeIndividualTracks(sample_rate=16000, output_path='../Audio/XTrack/'):\n",
    "    track_id = 1\n",
    "    for i, l in enumerate(predicted_labels):\n",
    "        if l == 'music':\n",
    "            start = predicted_onsets[i]\n",
    "            start_idx = int(start * sample_rate)\n",
    "            if i < len(predicted_labels) - 1:\n",
    "                stop = predicted_onsets[i+1]\n",
    "                stop_idx = int(stop * sample_rate)\n",
    "            else:\n",
    "                stop_idx = -1\n",
    "            track = waveform[start_idx:stop_idx]\n",
    "\n",
    "            wavfile.write(output_path+filename[:-4]+'_track'+str(track_id)+'.wav', sample_rate, track)\n",
    "            track_id += 1       \n",
    "\n",
    "def rounddown(x):\n",
    "    return int(math.floor(x / 1000.0)) * 1000\n",
    "\n",
    "def writeIndexesCSV(original_sample_rate = 48000, output_path='../Data/Markers/'):\n",
    "    with open(output_path+filename+'.csv', 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter='\t',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        csvwriter.writerow(['Name','Start','Duration','Time Format','Type','Description'])\n",
    "        for o,l in zip(predicted_onsets, predicted_labels):#, all_durations):\n",
    "            sample = int(o*original_sample_rate)\n",
    "            csvwriter.writerow([l,rounddown(sample),'0','48000 Hz','Cue','autoMarker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_audio_file = ''\n",
    "datapaths = [input_audio_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = '/Volumes/MJF_SAMPLE/2022/'\n",
    "\n",
    "datapaths = []\n",
    "for p in Path(input_folder).rglob('*MLAB3*.wav'):\n",
    "    datapaths.append(p)\n",
    "for p in Path(input_folder).rglob('*MLAB4*.wav'):\n",
    "    datapaths.append(p)\n",
    "len(datapaths) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22MLAB31A11BR.v0-fr128-sstereo.wav\n",
      "Sample rate: 16000 Hz\n",
      "Total duration: 2950.44s\n",
      "Size of the input: 47207040\n",
      "22MLAB36A11BR.v0-fr128-sstereo.wav\n",
      "Sample rate: 16000 Hz\n",
      "Total duration: 3210.36s\n",
      "Size of the input: 51365760\n",
      "22MLAB33A11BR.v0-fr128-sstereo.wav\n",
      "Sample rate: 16000 Hz\n",
      "Total duration: 3580.64s\n",
      "Size of the input: 57290240\n",
      "22MLAB34A11BR.v0-fr128-sstereo.wav\n",
      "Sample rate: 16000 Hz\n",
      "Total duration: 3445.76s\n",
      "Size of the input: 55132160\n",
      "22MLAB35A11BR.v0-fr128-sstereo.wav\n",
      "Sample rate: 16000 Hz\n",
      "Total duration: 3611.36s\n",
      "Size of the input: 57781760\n",
      "22MLAB32A11BR.v0-fr128-sstereo.wav\n",
      "Sample rate: 16000 Hz\n",
      "Total duration: 2718.24s\n",
      "Size of the input: 43491840\n",
      "22MLAB37A11BR.v0-fr128-sstereo.wav\n",
      "Sample rate: 16000 Hz\n",
      "Total duration: 3232.36s\n",
      "Size of the input: 51717760\n",
      "22MLAB38A11BR.v0-fr128-sstereo.wav\n",
      "Sample rate: 16000 Hz\n",
      "Total duration: 3090.96s\n",
      "Size of the input: 49455360\n",
      "22MLAB30A11BR.v0-fr128-sstereo.wav\n",
      "Sample rate: 16000 Hz\n",
      "Total duration: 2630.80s\n",
      "Size of the input: 42092800\n",
      "22MLAB41A11BR.v0-fr128-sstereo.wav\n",
      "Sample rate: 16000 Hz\n",
      "Total duration: 3468.96s\n",
      "Size of the input: 55503360\n",
      "22MLAB43A11BR.v0-fr128-sstereo.wav\n",
      "Sample rate: 16000 Hz\n",
      "Total duration: 5106.20s\n",
      "Size of the input: 81699200\n"
     ]
    }
   ],
   "source": [
    "output_path = '/Volumes/XTrack/'\n",
    "output_folder_name = input_folder.split('/')[-2]+'/'\n",
    "output_path = output_path+output_folder_name\n",
    "# create_directory(output_path)\n",
    "# audio_output_path = output_path+'Audio/'\n",
    "# create_directory(audio_output_path)\n",
    "# csv_output_path = output_path+'CSV/'\n",
    "# create_directory(csv_output_path)\n",
    "\n",
    "\n",
    "for d in datapaths:\n",
    "    filename = str(p).split('/')[-1]\n",
    "    filename, waveform, spectrogram_np, probs = processAudioFile(d)\n",
    "    predicted_onsets, predicted_labels = Xtrack(probs)\n",
    "    writeIndividualTracks(output_path=audio_output_path)\n",
    "    writeIndexesCSV(output_path=csv_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
